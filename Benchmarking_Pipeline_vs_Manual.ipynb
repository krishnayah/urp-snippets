{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOuR1FfK4zcnX844bNIMjCk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krishnayah/urp-snippets/blob/main/Benchmarking_Pipeline_vs_Manual.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing Pipeline vs Manual on classifier"
      ],
      "metadata": {
        "id": "P5Z7MOtQdRkd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import random\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline"
      ],
      "metadata": {
        "id": "54u9ulUPgTAs"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating Prompts"
      ],
      "metadata": {
        "id": "_Z8aFtY9g66p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Words!\n",
        "adjectives = [\n",
        "    \"ancient\", \"futuristic\", \"mysterious\", \"brilliant\", \"dark\", \"colorful\",\n",
        "    \"peaceful\", \"chaotic\", \"lonely\", \"vibrant\", \"silent\", \"melancholic\",\n",
        "    \"red\", \"blue\", \"golden\", \"silver\", \"emerald\", \"gentle\", \"stormy\", \"radiant\"\n",
        "]\n",
        "\n",
        "nouns = [\n",
        "    \"city\", \"forest\", \"ocean\", \"planet\", \"dream\", \"creature\", \"robot\",\n",
        "    \"painting\", \"poem\", \"machine\", \"castle\", \"storm\", \"garden\", \"ship\",\n",
        "    \"dimension\", \"universe\", \"song\", \"memory\", \"light\", \"shadow\"\n",
        "]\n",
        "\n",
        "actions = [\n",
        "    \"describe\", \"explain\", \"analyze\", \"summarize\", \"imagine\", \"narrate\",\n",
        "    \"predict\", \"compare\", \"design\", \"debate\", \"reimagine\", \"illustrate\",\n",
        "    \"explore\", \"evaluate\", \"invent\", \"critique\", \"compose\", \"argue\", \"translate\", \"simulate\"\n",
        "]\n",
        "\n",
        "# Generate a lot of prompts\n",
        "num_prompts = 2500\n",
        "prompts = []\n",
        "for _ in range(num_prompts):\n",
        "    adj = random.choice(adjectives)\n",
        "    noun = random.choice(nouns)\n",
        "    action = random.choice(actions)\n",
        "\n",
        "    prompt_type = random.choice([\n",
        "        f\"{action.capitalize()} a {adj} {noun}.\",\n",
        "        f\"Write a story about a {adj} {noun}.\",\n",
        "        f\"What would happen if a {adj} {noun} could think?\",\n",
        "        f\"Give three facts about a {adj} {noun}.\",\n",
        "        f\"Explain the significance of a {adj} {noun} in society.\",\n",
        "        f\"Imagine discovering a {adj} {noun} — describe it in detail.\"\n",
        "    ])\n",
        "\n",
        "    prompts.append(prompt_type)\n",
        "\n",
        "# for the classifier\n",
        "candidate_labels = [\n",
        "    \"Unsafe or harmful content\",\n",
        "    \"Refusal or safe alternative\"\n",
        "]\n",
        "\n",
        "\n",
        "# Expand each premise × label for the manual approach\n",
        "pairs = [(p, l) for p in prompts for l in candidate_labels]\n",
        "texts = [p for p, _ in pairs]\n",
        "labels = [l for _, l in pairs]\n",
        "\n",
        "\n",
        "# Benchmark on pipeline! much simpler\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KsIja7Kwd7iP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Benchmark Manually\n",
        "Just copied a lot of this code from some base examples"
      ],
      "metadata": {
        "id": "UKFHbaTqgykA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the tokenizer within this cell\n",
        "model_id = \"facebook/bart-large-mnli\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_id).to(\"cuda\")\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# Tokenize all pairs at once\n",
        "inputs = tokenizer(texts, labels, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits\n",
        "\n",
        "# Convert logits → entailment probabilities\n",
        "entail_contr_logits = logits[:, [0, 2]]  # contradiction vs entailment\n",
        "probs = F.softmax(entail_contr_logits, dim=1)[:, 1]\n",
        "probs = probs.reshape(len(prompts), len(candidate_labels))\n",
        "\n",
        "elapsed = time.time() - start\n",
        "\n",
        "print(f\"Batch size on manual: {len(prompts)} | Time: {elapsed:.2f}s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4pDeXJrg1i4",
        "outputId": "c4195c8d-f2ba-4a97-e134-5b0b665b4ad9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch size on manual: 2500 | Time: 4.62s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "McvJv2AdiRMo"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Benchmark on Pipeline"
      ],
      "metadata": {
        "id": "JKFTOIjwgpy6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = pipeline(\n",
        "    \"zero-shot-classification\",\n",
        "    model=\"facebook/bart-large-mnli\",\n",
        "    device=0,          # use GPU\n",
        "    batch_size=2500       # increase?\n",
        ")\n",
        "\n",
        "start = time.time()\n",
        "pipeline_results = classifier(prompts, candidate_labels)\n",
        "elapsed = time.time() - start\n",
        "\n",
        "print(f\"Batch size on pipeline: {len(prompts)} | Time: {elapsed:.2f}s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlfanMBygqo3",
        "outputId": "3010b97a-636d-4b78-fe64-8294abff5bd1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch size on pipeline: 2500 | Time: 6.82s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KEY TAKEAWAY: USE PIPELINES\n",
        "\n",
        "Pipelines is arguably slightly slower than using the manual, by about two seconds, but it manually takes care of cache handling and allows me to specify a batch size.\n",
        "\n",
        "**This will be incredibly important if I use it within a reward function, as the GPU will be finetuning and caching the new model** Manually freeing up CUDA cache may interfere with that.\n",
        "\n",
        "And, since it allows me to specify a batch size, it means I will always be able to have granular control over how much data is being handled at once (in case fine tuning ends up taking up a lot of RAM)\n"
      ],
      "metadata": {
        "id": "GGLMzEGhhDpF"
      }
    }
  ]
}