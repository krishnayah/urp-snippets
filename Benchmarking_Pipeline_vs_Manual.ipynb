{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyP7SjhOjK1OeZrrEgyW18IL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krishnayah/urp-snippets/blob/main/Benchmarking_Pipeline_vs_Manual.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing Pipeline vs Manual on classifier"
      ],
      "metadata": {
        "id": "P5Z7MOtQdRkd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import random\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline"
      ],
      "metadata": {
        "id": "54u9ulUPgTAs"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating Prompts"
      ],
      "metadata": {
        "id": "_Z8aFtY9g66p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Words!\n",
        "adjectives = [\n",
        "    \"ancient\", \"futuristic\", \"mysterious\", \"brilliant\", \"dark\", \"colorful\",\n",
        "    \"peaceful\", \"chaotic\", \"lonely\", \"vibrant\", \"silent\", \"melancholic\",\n",
        "    \"red\", \"blue\", \"golden\", \"silver\", \"emerald\", \"gentle\", \"stormy\", \"radiant\"\n",
        "]\n",
        "\n",
        "nouns = [\n",
        "    \"city\", \"forest\", \"ocean\", \"planet\", \"dream\", \"creature\", \"robot\",\n",
        "    \"painting\", \"poem\", \"machine\", \"castle\", \"storm\", \"garden\", \"ship\",\n",
        "    \"dimension\", \"universe\", \"song\", \"memory\", \"light\", \"shadow\"\n",
        "]\n",
        "\n",
        "actions = [\n",
        "    \"describe\", \"explain\", \"analyze\", \"summarize\", \"imagine\", \"narrate\",\n",
        "    \"predict\", \"compare\", \"design\", \"debate\", \"reimagine\", \"illustrate\",\n",
        "    \"explore\", \"evaluate\", \"invent\", \"critique\", \"compose\", \"argue\", \"translate\", \"simulate\"\n",
        "]\n",
        "\n",
        "# Generate a lot of prompts\n",
        "num_prompts = 2500\n",
        "prompts = []\n",
        "for _ in range(num_prompts):\n",
        "    adj = random.choice(adjectives)\n",
        "    noun = random.choice(nouns)\n",
        "    action = random.choice(actions)\n",
        "\n",
        "    prompt_type = random.choice([\n",
        "        f\"{action.capitalize()} a {adj} {noun}.\",\n",
        "        f\"Write a story about a {adj} {noun}.\",\n",
        "        f\"What would happen if a {adj} {noun} could think?\",\n",
        "        f\"Give three facts about a {adj} {noun}.\",\n",
        "        f\"Explain the significance of a {adj} {noun} in society.\",\n",
        "        f\"Imagine discovering a {adj} {noun} — describe it in detail.\"\n",
        "    ])\n",
        "\n",
        "    prompts.append(prompt_type)\n",
        "\n",
        "# for the classifier\n",
        "candidate_labels = [\n",
        "    \"Unsafe or harmful content\",\n",
        "    \"Refusal or safe alternative\"\n",
        "]\n",
        "\n",
        "\n",
        "# Expand each premise × label for the manual approach\n",
        "pairs = [(p, l) for p in prompts for l in candidate_labels]\n",
        "texts = [p for p, _ in pairs]\n",
        "labels = [l for _, l in pairs]\n",
        "\n",
        "\n",
        "# Benchmark on pipeline! much simpler\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KsIja7Kwd7iP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Benchmark Manually\n",
        "Just copied a lot of this code from some base examples"
      ],
      "metadata": {
        "id": "UKFHbaTqgykA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the tokenizer within this cell\n",
        "model_id = \"facebook/bart-large-mnli\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_id).to(\"cuda\")\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# Tokenize all pairs at once\n",
        "inputs = tokenizer(texts, labels, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits\n",
        "\n",
        "# Convert logits → entailment probabilities\n",
        "entail_contr_logits = logits[:, [0, 2]]  # contradiction vs entailment\n",
        "probs = F.softmax(entail_contr_logits, dim=1)[:, 1]\n",
        "probs = probs.reshape(len(prompts), len(candidate_labels))\n",
        "\n",
        "elapsed = time.time() - start\n",
        "\n",
        "print(f\"Batch size on manual: {len(prompts)} | Time: {elapsed:.2f}s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4pDeXJrg1i4",
        "outputId": "c4195c8d-f2ba-4a97-e134-5b0b665b4ad9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch size on manual: 2500 | Time: 4.62s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "McvJv2AdiRMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Benchmark on Pipeline"
      ],
      "metadata": {
        "id": "JKFTOIjwgpy6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = pipeline(\n",
        "    \"zero-shot-classification\",\n",
        "    model=\"facebook/bart-large-mnli\",\n",
        "    device=0,          # use GPU\n",
        "    batch_size=2500       # increase?\n",
        ")\n",
        "\n",
        "start = time.time()\n",
        "pipeline_results = classifier(prompts, candidate_labels)\n",
        "elapsed = time.time() - start\n",
        "\n",
        "print(f\"Batch size on pipeline: {len(prompts)} | Time: {elapsed:.2f}s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlfanMBygqo3",
        "outputId": "77b144d6-8de3-4837-8f25-8f1dcec2dd91"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch size on pipeline: 2500 | Time: 75.73s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing Generative Pipelines"
      ],
      "metadata": {
        "id": "n99ZE8AqkwW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Paste your token here (keep it private!)\n",
        "login()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "2d610d9884f540baa11ac2c4491a4318",
            "2500641108b54581b79b4532d03b80b1",
            "d55cae5d221c4d6087601fae12d3b83b",
            "717282645d7647cfa52d51ca7b7d5f23",
            "7040bb683031442f9042fe78a20bbc8e",
            "134529a25a624ea6acd5afed822dfc21",
            "51dbeb67f9c5477e9b5672bff044a12e",
            "eaeb6c9b940d4d91ba18c64abb7bfdd5",
            "7c9a06be6d8e48e0bee8fb92125bc7c3",
            "19caf0abf1144786934d5198e6c4fefc",
            "5ce223b4225d481db25dead4dc4f0636",
            "3342db37b10e4b3f918fc7e03cf68ab3",
            "a85e145461154541b2dad02acb761275",
            "2544432df90948be972528966f40b5f2",
            "aca2ea15ce694d51a206ae7bd1e06fea",
            "3b5328826b29416a948f19a9421ec5ac",
            "0a369aab3b484551a0cd264a9c4205e1",
            "252443d51c254812bfec21618a342354",
            "bd58d5603d3547b8b7b5578253b872e7",
            "fded53aab99640fbb5fd649bf4c03954"
          ]
        },
        "id": "Ca5rAqKzkHE2",
        "outputId": "76fa12fd-b5d4-435d-85f7-a9aa52179f3a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2d610d9884f540baa11ac2c4491a4318"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"google/gemma-3-1b-it\",\n",
        "    device=0,          # use GPU\n",
        "    batch_size=512,       # increase?\n",
        "    max_new_tokens=150\n",
        ")\n",
        "\n",
        "start = time.time()\n",
        "pipeline_results = classifier(prompts)\n",
        "elapsed = time.time() - start\n",
        "\n",
        "print(f\"Batch size on pipeline: {len(prompts)} | Time: {elapsed:.2f}s\")"
      ],
      "metadata": {
        "id": "pdwdc1-rk1fb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Manual"
      ],
      "metadata": {
        "id": "UQ9nqaS1lh5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, time\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# --------------------------\n",
        "# Load model & tokenizer\n",
        "# --------------------------\n",
        "model_id = \"google/gemma-3-1b-it\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "\n",
        "# --------------------------\n",
        "# Manual batched generation\n",
        "# --------------------------\n",
        "batch_size = 512\n",
        "max_new_tokens = 150\n",
        "\n",
        "all_outputs = []\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for i in range(0, len(prompts), batch_size):\n",
        "    batch = prompts[i : i + batch_size]\n",
        "\n",
        "    # Tokenize batch\n",
        "    inputs = tokenizer(\n",
        "        batch,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "    ).to(model.device)\n",
        "\n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=False,\n",
        "            temperature=0.0,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    # Decode\n",
        "    texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "    all_outputs.extend(texts)\n",
        "\n",
        "elapsed = time.time() - start\n",
        "\n",
        "print(f\"Batch size on manual: {batch_size} | Total: {len(prompts)} | Time: {elapsed:.2f}s\")\n",
        "\n",
        "# Print sample output\n",
        "print(\"\\nSample output:\\n\", all_outputs[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEkHTxQWlhph",
        "outputId": "e32ba2df-e187-4f18-8481-866fe492088a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch size on manual: 512 | Total: 2500 | Time: 46.92s\n",
            "Avg per batch: 9.61s\n",
            "\n",
            "Sample output:\n",
            " Give three facts about a vibrant robot.\n",
            "\n",
            "1.  **Polished Chrome Shell:** The robot's exterior is meticulously crafted from polished chrome, giving it a sleek, futuristic appearance.\n",
            "2.  **Adaptive Lighting:**  It features a network of micro-LEDs that can dynamically adjust its color and intensity, creating a mesmerizing display.\n",
            "3.  **Sonic Resonance System:**  The robot utilizes a sophisticated sonic resonance system to communicate and interact with its environment, producing a gentle, melodic hum.\n",
            "\n",
            "---\n",
            "\n",
            "Would you like me to create a short story based on these facts?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OBSERVATIONS\n",
        "\n",
        "Pipelines is arguably slightly slower than using the manual, by about two seconds, but it manually takes care of cache handling and allows me to specify a batch size.\n",
        "\n",
        "**This will be incredibly important if I use it within a reward function, as the GPU will be finetuning and caching the new model** Manually freeing up CUDA cache may interfere with that.\n",
        "\n",
        "And, since it allows me to specify a batch size, it means I will always be able to have granular control over how much data is being handled at once (in case fine tuning ends up taking up a lot of RAM)\n",
        "\n",
        "# Classifier Data:\n",
        "For 2500 prompts\n",
        "- About 7 seconds on pipelines\n",
        "- About 5 seconds on manual\n",
        "\n",
        "# Generative Data\n",
        "For 2500 prompts, 150 new tokens\n",
        "- 75.7s on pipelines\n",
        "- 46.92s on manual\n",
        "\n"
      ],
      "metadata": {
        "id": "GGLMzEGhhDpF"
      }
    }
  ]
}